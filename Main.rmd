---
title: "ST404-Project-1"
author: "group17"
date: "27/01/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Which variables show a strong relationship with the outcome variables?


To see which variables are significantly associated with the outcome variables, we will conduct hypothesis tests on each of the variables. The hypotheses are the following: 


$$H_0: \text{corr}(x_i,y_j)=0, H_1: \text{corr}(x_i,y_j)\neq 0 , i=1,...,20, j=1,2$$

We explain $i$ ranging in the interval $1$ to $20$ due to there being $24$ variables, out of which $2$ are factor variables and $2$ are target variables.

In the following chunk of code we are going to conduct tests with the variable `violentPerPop`:

```{r}
# Subsetting the dataframe so that it excludes State, region and violentPerPop
dfcor1 <- subset(df, select = -c(State, region, violentPerPop,nonViolPerPop))

for(i in colnames(dfcor1)){
  print(i)
  test=cor.test(dfcor1[[i]],df$violentPerPop)
 cat(sprintf("t-statistic is: %s, p-value is: %s, and correlation coefficient estimate is %s",test$statistic, test$p.value,test$estimate))
  writeLines("\n")
}

```

As we can see, the only variable which doesn't have correlation which is significantly different than $0$ is `pctVacant6up`. We also observe very strong correlation with the variables `pctKids2Par` as well as `pctKidsBornNevrMarr`.

We will now examine the relationship between `nonViolPerPop` and the other variables:

```{r}
for(i in colnames(dfcor1)){
  print(i)
  test=cor.test(dfcor1[[i]],df$nonViolPerPop)
  cat(sprintf("t-statistic is: %s, p-value is: %s, and correlation coefficient estimate is %s",test$statistic, test$p.value,test$estimate))
  writeLines("\n")
}
```
The variables `pctUrban` and `pctVacant6upp` don't have correlations which are significantly different than $0$. We also observe `pctKids2Par` and `pctKidsBornNevrMarr` having strong correlation with the respective target.


Having tested on both targets, we can say that the variables `pctKids2Par` and `pctKidsBornNevrMarr` correlate strongly with the targets.

## Can the relationship be characterized as a linear?

We will plot the variables `pctKids2Par`, `pctKidsBornNevrMarr`, `nonViolPerPop` and `violentPerPop` visualizing them in scatterplots:

```{r}
pairs(df[,c(12,11,23,24)])
```

The scatterplots indicate that the `pctKidsBornNevrMarr` and `pctKids2Par` variables may have a linear relationship with each other, although it is not clear whether these variables have linear relationships with the targets.

To see whether we can classify the relationship between these variables as linear, we can fit a linear and nonlinear model and asses whether the nonlinear model explains a significantly larger amount of variance via ANOVA.

We will now test whether the variable `pctKidsBornNevrMarr` performs better with a linear or nonlinear model when modeling for `violentPerPop`:

```{r}
linModel=lm(df$violentPerPop~df$pctKidsBornNevrMarr)
sqModel=lm(df$violentPerPop~poly(df$pctKidsBornNevrMarr,2))

anova(linModel,sqModel)
```

We can see that the p-value$=2.223e-06 \lt 0.001$. So having a non-linear model fitted for the variables `violentPerPop` and `pctKidsBornNevrMarr` did lead to a significantly improved fit over the linear model. 

We now fit the same models for the variables `pctKidsBornNevrMarr` and `nonViolPerPop`:

```{r}
linModel=lm(df$nonViolPerPop~df$pctKidsBornNevrMarr)
sqModel=lm(df$nonViolPerPop~poly(df$pctKidsBornNevrMarr,2))

anova(linModel,sqModel)
```
We again see that the non-linear model leads to a significantly improved fit over the linear model.

We now test for the variables `pctKids2Par` and `nonViolPerPop`:

```{r}
linModel=lm(df$nonViolPerPop~df$pctKids2Par)
sqModel=lm(df$nonViolPerPop~poly(df$pctKids2Par,2))

anova(linModel,sqModel)
```
The result shows a non-significant result, $p=0.6419$. Thus, we fail to accept that the non-linear model provides a better fit and hence the relationship is linear.

And lastly we test the variables `pctKids2Par` and `violentPerPop`:

```{r}
linModel=lm(df$violentPerPop~df$pctKids2Par)
sqModel=lm(df$violentPerPop~poly(df$pctKids2Par,2))

anova(linModel,sqModel)
```
The non-linear model provides a significant improvement in fit.

We can conclude that the only relationship which is close to linear is the relationship between the variables `pctKids2Par` and `nonViolPerPop`. 

## Does the relationship appear to be homoscedastic?

To test whether the  relationships are homescedastic, we are going to perform the Breusch-Pagan test with the `bptest()` function.

We will test for the variables `pctKidsBornNevrMarr` and `violentPerPop`:



```{r}
install.packages("lmtest")
library(lmtest)
model=lm(violentPerPop~pctKidsBornNevrMarr,data=df)
bptest(model)
```
The p-value is 2.2e-16, hence less than 0.05. This means we have sufficient evidence that there is heteroscedasticity present in the model.

Testing for the variables `pctKidsBornNevrMarr` and `nonViolPerPop`:


```{r}
model=lm(nonViolPerPop~pctKidsBornNevrMarr,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.

We test for `pctKids2Par` and `violentPerPop`:

```{r}
model=lm(violentPerPop~pctKids2Par,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.

We test for `pctKids2Par` and `nonViolPerPop`:

```{r}
model=lm(nonViolPerPop~pctKids2Par,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.


In all of the relationships that we have examined for presence of homoscedasticity, we have found homoscedasticity present in every relationship. 


# What transformations, if any, might be applied, to resolve any issues? 

If heteroscedasticity is the issue we can apply a log transformation on the targets.


# Are there any other approaches that could be taken to tackle these issues?
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  2) Which variables, if any, have a highly skewed distribution? What transformations might be
  applied to reduce skewness and stabilize the spread of the observations?
  3) Do any of the variables have outlying values? How should outliers be treated?
  4) Which variables are highly correlated with each other? Are there variables that represent
  different ways of measuring the same thing?
  5) Given all of the above, what recommendations would you suggest for preparing these data
  in order to fit a linear model?


TEST CHANGE11







# Correlation between explanatory variables

To investigate the correlation between all explanatory variables, we first plot a correlation matrix with all explanatory variables included (In Appendix).

However, a correlation matrix with 20 variables is difficult to look at. We can remove variables that have low correlation with all other variables, that are variables that have absolute value of correlation coefficient of less than or equal to 0.65 with all other variables, to remove some variables out of the discussion. 

We are then left with the following correlation matrix:

```{r figs, echo=FALSE, fig.width=5,fig.height=4,fig.cap="\\label{fig:figs}Correlation Matrix for selected explanatory variables"}
USACrimeCorrelation <- subset(USACrime,select=-c(State,region,violentPerPop,nonViolPerPop,pctUrban,pctHousOccup,pctHousOwnerOccup,pctVacantBoarded,pctVacant6up,popDensity,pctForeignBorn))
Correlation <- cor(USACrimeCorrelation, use="pairwise.complete.obs")

pvaluematrix <- cor.mtest(USACrimeCorrelation)$p

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(Correlation, method = "color", col = col(200),
         type = "lower", number.cex = 0.5, 
         addCoef.col = "black", tl.cex = 0.6,
         tl.col = "black", tl.srt = 90, 
         p.mat = pvaluematrix, sig.level = 0.01, insig = "blank", 
         diag = FALSE)
```

Using this correlation matrix, we are able to identify pairs or clusters of explanatory variables that are highly correlated to each other. Then we can pick one that best represents the pair/cluster and remove the rest during model building. During this part of the investigation, we would assume that two variables are highly correlated if the absolute value for their correlation coefficient is larger than 0.65.

1. The first pair of highly correlated variables is 
  + **`medIncome`**, median household income
  + **`pctWdiv`**, percentage of households with investment/rent income
  
|       Both variables measures how wealthy people are in the community. Hence, we recommend using **`medIncome`** as the only wealth measuring variable.
\

2. The second cluster of highly correlated variables is
  + **`pctLowEdu`**, percentage of people 25 and over with less than a 9th grade education
  + **`pctNotHSgrad`**, percentage of people 25 and over that are not high school graduates
  + **`pctCollGrad`**,percentage of people 25 and over with a bachelors degree or higher education
  
|       All three variables measures the education level of the people in the community. Hence, we recommend using **`pctLowEdu`** as the only education measuring variable.
\

3. The third pair of highly correlated variables is
  + **`pctUnemployed`**, percentage of people 16 and over, in the labor force, and unemployed
  + **`pctEmploy`**, percentage of people 16 and over who are employed

|       Both variables measures the employability of the people in the community. Hence, we recommend using **`pctUnemployed`** as the only employment measuring variable.
\


4. The forth pair of highly correlated variables is 
  + **`pctKids2Par`**, percentage of kids in family housing with two parents
  + **`pctKidsBornNevrMarr`**, percentage of kids born to never married
  
|       Both variables measures the completeness of family of the people in the community. Hence, we recommend using **`pctKids2Par`** as the only completeness of family measuring variable.
\

5. The last cluster of highly correlated variables is
  + **`ownHouseMed`**, owner occupied housing - median value
  + **`ownHousQrange`**, owner occupied housing - difference between upper quartile and lower quartile values
  + **`rentMed`**, rental housing - median rent

|       All three variables measures the cost of living(housing) for the people in the community. Hence, we recommend using **`ownHouseMed`** as the only completeness of family measuring variable.

In the end, we recommend using these remaining variables for model building purposes.








# Appendix

```{r echo=FALSE}
USACrimeCorrelation <- subset(USACrime, select=-c(State,region,violentPerPop,nonViolPerPop))
Correlation <- cor(USACrimeCorrelation, use="pairwise.complete.obs")

corrplot(Correlation, type="lower", method="number", number.cex=0.5, tl.col="black")
```
